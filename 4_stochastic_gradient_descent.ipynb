{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vietnamese-trust",
   "metadata": {},
   "source": [
    "# Notebook 2: Stochastic Gradient Descent\n",
    "\n",
    "Gradient descent is the workhorse of machine learning. The goal of this notebook is to develop the basic algorithms in the context of two common problems: a simple linear regression and logistic regression for binary classification. \n",
    "\n",
    "This notebook was written by [Joe Boyd](https://github.com/jcboyd), [Judith Abecassis](https://github.com/judithabk6) and [Chlo√©-Agathe Azencott](http://cazencott.info), with inspiration from [the AM207 course at Harvard](http://am207.github.io/2016/) and [Francis Bach](http://www.di.ens.fr/%7Efbach/orsay2018.html).\n",
    "\n",
    "\n",
    "__If you are taking this class for credit, you will need to turn in your completed notebook on Campus.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using vector graphics to obtain crisper plots\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the colors that matplotlib cycles on\n",
    "# to be able to use them directly\n",
    "# see: https://matplotlib.org/stable/gallery/color/color_cycle_default.html\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "notebook_colors = prop_cycle.by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-physics",
   "metadata": {},
   "source": [
    "## 1. Linear regression\n",
    "\n",
    "In this section, we will solve a simple linear regression between two variables: the number of features is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-blast",
   "metadata": {},
   "source": [
    "### 1.1 Generating data\n",
    "\n",
    "Here we use the `make_regression` method of `sklearn.datasets` ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)) to generate our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "n_features = 1\n",
    "X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, n_informative=1, noise=20, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-voluntary",
   "metadata": {},
   "source": [
    "Let us visualize the data we have generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (output)\")\n",
    "plt.title(\"Synthetic data set\")\n",
    "\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-merchandise",
   "metadata": {},
   "source": [
    "### 1.2 Optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-contents",
   "metadata": {},
   "source": [
    "Given a data matrix of $n$ samples in $p$ dimensions $X \\in \\mathbb{R}^{nxp}$ (in our simulation, $p=1$) and an output vector $\\boldsymbol{y} \\in \\mathbb{R}^{n}$, the objective of a linear regression is to learn a linear function,\n",
    "\n",
    "$$f: \\boldsymbol{x} \\in \\mathbb{R}^p \\mapsto \\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{x}$$\n",
    "\n",
    "where $\\beta_0 \\in \\mathbb{R}$ and $\\boldsymbol\\beta \\in \\mathbb{R}^p$.\n",
    "\n",
    "This is achieved by __minimizing the empirical risk__, computed with the quadratic error as loss function: $\\frac1n \\sum_{i=1}^n \\left(y_i - f(\\boldsymbol{x}_i)\\right)^2$. \n",
    "\n",
    "The $\\beta_0$ parameter corresponds to an intercept (or bias) term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-poker",
   "metadata": {},
   "source": [
    "#### Bias trick\n",
    "To simplify the equations, we append a column of 1s to the matrix $X$. \n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "    x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\dots & x_{np}\n",
    "\\end{bmatrix}}_{X} \\to\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "    1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
    "\\end{bmatrix}}_{\\text{$X$ with bias variable}}\n",
    "$$\n",
    "\n",
    "Now we can simply use \n",
    "$f: \\boldsymbol{x} \\in \\mathbb{R}^{p+1} \\mapsto \\boldsymbol{\\beta}^\\top \\boldsymbol{x}$ with $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}$.\n",
    "\n",
    "This is called the __bias trick.__\n",
    "\n",
    "Let us now compute the numpy array `X_bt` that corresponds to our data matrix `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bt = np.hstack((np.ones((n_samples, 1)), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-monster",
   "metadata": {},
   "source": [
    "We will now work with matrix `X_bt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-dress",
   "metadata": {},
   "source": [
    "#### Closed-form solution\n",
    "\n",
    "The empirical risk minimization can be rewritten as:\n",
    "$$\\boldsymbol{\\beta}^* \\in \\arg\\min \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\boldsymbol{\\beta}^\\top \\boldsymbol{x}_i \\right)^2 = \\frac{1}{n}\\left(\\boldsymbol{y} - X \\boldsymbol\\beta\\right)^\\top(\\boldsymbol{y} - X\\boldsymbol\\beta)$$\n",
    "\n",
    "This is a convex optimization problem, so to solve it we can differentiate the function to minimize and set its gradient to zero.\n",
    "\n",
    "We obtain \n",
    "$(X^\\top X) \\boldsymbol\\beta^* =  X^\\top \\boldsymbol{y},$\n",
    "\n",
    "which, __if $X^\\top X$ can be inverted__, leads to a unique solution\n",
    "$\\boldsymbol\\beta^* = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}.$\n",
    "\n",
    "If $X^\\top X$ cannot be inverted, the above problem has an infinity of solutions. They can also be expressed analytically by replacing $(X^\\top X)^{-1}$ with a [pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $X^\\top X.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-hungary",
   "metadata": {},
   "source": [
    "__Instruction:__ Use `np.linalg.inv` ([doc](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)) to compute the vector `beta_star` that is solution to the ordinary least squares in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_star = # TODO\n",
    "print(beta_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-progress",
   "metadata": {},
   "source": [
    "#### Quality of the model\n",
    "\n",
    "We can now evaluate the quality of the fit of the closed-form solution model. Note that here we are only concerned with fitting the data, and not about generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-equity",
   "metadata": {},
   "source": [
    "__Instruction:__ Compute the vector `y_pred` of outputs predicted by the obtained model on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-corrections",
   "metadata": {},
   "source": [
    "We can now visualize the regression line we have just learned, and plot the predicted values of $y$ versus the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (output)\")\n",
    "plt.title(\"Synthetic data set with regression line\")\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, lw=2, color=notebook_colors[1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"true output\")\n",
    "plt.ylabel(\"predicted output\")\n",
    "plt.xlim([-200, 200])\n",
    "plt.ylim([-200, 200])\n",
    "plt.title(\"Quality of predictions\")\n",
    "plt.scatter(y, y_pred)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-breast",
   "metadata": {},
   "source": [
    "We can also quantify the error with the RMSE, or Root Mean Squared Error, defined by \n",
    "$$\\sqrt{\\sum_{i=1}^n \\frac1n \\left(y_i - f(\\boldsymbol{x}_i)\\right)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-timing",
   "metadata": {},
   "source": [
    "__Instruction:__ Complete the code below to compute the RMSE of the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_predicted):\n",
    "    return # TODO\n",
    "\n",
    "print('RMSE: %.2f' % root_mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-cream",
   "metadata": {},
   "source": [
    "#### Relationship between noise and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-conservative",
   "metadata": {},
   "source": [
    "__Instruction:__ Complete the code below to compute the RMSE of a linear regression fitted to data as a function of the value of the `noise` parameter. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_noise(noise_value):\n",
    "    \"\"\"\n",
    "    Return the RMSE of a univariate linear regression on data generated with noise parameter noise_value.\n",
    "    \"\"\"\n",
    "    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, n_informative=1, noise=noise_value, random_state=37)\n",
    "    X_bt = np.hstack((np.ones((n_samples, 1)), X))\n",
    "    beta_star = # TODO\n",
    "    y_pred = # TODO\n",
    "    return(root_mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.xlabel(\"Noise in the data\")\n",
    "plt.ylabel(\"RMSE of the linear regression\")\n",
    "\n",
    "noise_values = # TODO\n",
    "plt.scatter(noise_values, [play_with_noise(x) for x in noise_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-timber",
   "metadata": {},
   "source": [
    "### 1.3 Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-karaoke",
   "metadata": {},
   "source": [
    "It will not always be possible to use a closed-form solution of the empirical risk minimization problem.\n",
    "\n",
    "In the case of linear regression, there are two reasons why we might not want to use the closed-form solution:\n",
    "* if $X^\\top X$ cannot be inverted (which happens if some of the features are correlated, or if the number of features is larger than the number of columns);\n",
    "* if $X^\\top X$ is invertible but of large dimension, ie. when the number of features is large. Indeed, matrix inversion is _cubic_ in the dimension of the matrix.\n",
    "\n",
    "When the empirical risk minimization problem is convex, __gradient descent__ finds a solution by progressing iteratively in the direction opposite the gradient. Specifically, at each iteration $t$, the vector $\\boldsymbol\\beta$ is updated by:\n",
    "$$\\boldsymbol\\beta^{(t+1)} = \\boldsymbol\\beta^{(t)} - \\alpha \\nabla_{\\boldsymbol\\beta} J(\\boldsymbol\\beta^{(t)}),$$\n",
    "where $J: \\mathbb{R}^d \\rightarrow \\mathbb{R} $ is the function to minimize.\n",
    "\n",
    "In the case of the ordinary least squares, $d=p+1$ and $J(\\boldsymbol\\beta) = \\frac{1}{n}\\left(\\boldsymbol{y} - X \\boldsymbol\\beta\\right)^\\top(\\boldsymbol{y} - X\\boldsymbol\\beta).$\n",
    "\n",
    "$\\alpha$ is the step size of the gradient descent, also called the __learning rate.__\n",
    "\n",
    "The gradient descent is stopped either when a preset maximum number of iterations is reached, or when the norm of the gradient is less than a preset error level (called __tolerance__)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-butterfly",
   "metadata": {},
   "source": [
    "#### Gradient of the empirical risk of the linear regression\n",
    "For ordinary least squares, the gradient of the function to minimize is:\n",
    "\n",
    "$$\\nabla_{\\boldsymbol\\beta} J(\\boldsymbol\\beta) = \\frac1n X^\\top \\left(X \\boldsymbol\\beta - y \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-replica",
   "metadata": {},
   "source": [
    "__Instruction:__ Complete the code below to evaluate the gradient of the empirical risk of the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient(X, y, b_vector):\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-contractor",
   "metadata": {},
   "source": [
    "Let us check the value of the gradient at our solution `beta_star`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In the closed-form solution beta_star, the norm of the gradient is %.2e\" % (np.linalg.norm(evaluate_gradient(X_bt, y, beta_star))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-attempt",
   "metadata": {},
   "source": [
    "__Question:__ Does this value match your expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-device",
   "metadata": {},
   "source": [
    "#### Batch gradient descent\n",
    "\n",
    "In batch gradient descent, we use the entire data set at each iteration.\n",
    "\n",
    "__Instructions:__ Complete the `batch_gradient_descent` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(X, y, b_vector):\n",
    "    \"\"\"\n",
    "    Squared loss of a linear regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector.\n",
    "        \n",
    "    b_vector: (n_features, ) numpy array\n",
    "        The weight vector of the linear regression.    \n",
    "    \"\"\"\n",
    "    return(np.sum((y - X.dot(b_vector))**2)/np.size(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-plymouth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T17:38:54.314594Z",
     "start_time": "2019-03-27T17:38:54.288955Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, learning_rate=1e-1, max_iters=30, tol=1e-2):\n",
    "    \"\"\"\n",
    "    Batch gradient descent procedure for a linear regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector.\n",
    "        \n",
    "    learning_rate: float\n",
    "        The learning rate of the gradient descent.\n",
    "        \n",
    "    max_iters: int\n",
    "        The maximum number of iterations of the gradient descent.\n",
    "        \n",
    "    tol: float\n",
    "        The tolerance of the gradient descent:\n",
    "        the gradient descent will stop when the norm of the gradient is smaller than the tolerance.\n",
    "    \"\"\"\n",
    "    # Random initialisation of beta\n",
    "    beta_current = np.random.rand(# TODO)\n",
    "    \n",
    "    # Keep track of the values of the loss and of beta at each iteration\n",
    "    losses = [squared_loss(X, y, beta_current)]\n",
    "    betas = [beta_current.copy()]\n",
    "\n",
    "    for idx in range(max_iters):        \n",
    "        # update beta_current \n",
    "        beta_current = # TODO\n",
    "        \n",
    "        # compute the loss and append to losses\n",
    "        losses.append(squared_loss(X, y, beta_current))\n",
    "        \n",
    "        # append beta_current to losses\n",
    "        betas.append(beta_current.copy())\n",
    "\n",
    "        # check whether tolerance is reached\n",
    "        # (you can use np.linalg.norm to compute the norm of a vector)\n",
    "        if# TODO: \n",
    "            print(\"Gradient descent stopped after %d iterations\" % (idx+1))\n",
    "            break\n",
    "\n",
    "    return(np.array(losses), np.array(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-communications",
   "metadata": {},
   "source": [
    "We can now run the batch gradient descent on the data and observe how beta and the loss evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = 35\n",
    "lr = 2e-2\n",
    "losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses, 'o-')\n",
    "\n",
    "beta_star_loss = squared_loss(X_bt, y, beta_star)\n",
    "plt.plot([0, iterations], [beta_star_loss, beta_star_loss], 'k-', label=\"Optimal loss\")\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Batch gradient descent\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-sapphire",
   "metadata": {},
   "source": [
    "__Question:__ Has your gradient descent stopped because you've reached the maximum number of parameters or the tolerance? If the former, what parameter(s) do you think you could change to actually go below tolerance? Experiment below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-necessity",
   "metadata": {},
   "source": [
    "#### Quality of the model\n",
    "\n",
    "We can now evaluate the quality of the fit of the model learned by our gradient descent, and compare it to the model we obtained with the closed-form solution.\n",
    "\n",
    "__Instruction:__ Use the code below to look at the quality of a gradient descent that hasn't converged yet. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = 35\n",
    "lr = 2e-2\n",
    "losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gd = X_bt.dot(betas[-1])\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (output)\")\n",
    "plt.title(\"Synthetic data set with regression line\")\n",
    "plt.scatter(X, y, alpha=0.5)\n",
    "plt.plot(X, y_pred, lw=2, color=notebook_colors[1], label='Closed-form solution')\n",
    "plt.plot(X, y_pred_gd, lw=2, color=notebook_colors[2], label='Gradient descent solution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"true output\")\n",
    "plt.ylabel(\"predicted output\")\n",
    "plt.xlim([-200, 200])\n",
    "plt.ylim([-200, 200])\n",
    "plt.title(\"Quality of predictions\")\n",
    "plt.scatter(y, y_pred, label=\"Closed-form solution\", alpha=0.5)\n",
    "plt.scatter(y, y_pred_gd, label=\"Gradient descent solution\", alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-strategy",
   "metadata": {},
   "source": [
    "#### Gradient descent visualization\n",
    "\n",
    "You can use the following code to visualize the gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = 35\n",
    "lr = 5e-1\n",
    "losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-headline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T17:39:01.369834Z",
     "start_time": "2019-03-27T17:38:56.002975Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create mesh grid\n",
    "ms = np.linspace(beta_star[0] - 20 , beta_star[0] + 20, 20)\n",
    "bs = np.linspace(beta_star[1] - 80 , beta_star[1] + 80, 20)\n",
    "M, B = np.meshgrid(ms, bs)\n",
    "\n",
    "# Compute the squared loss in each point of the mesh grid\n",
    "zs = np.array([squared_loss(X_bt, y, theta)\n",
    "               for theta in zip(np.ravel(M), np.ravel(B))])\n",
    "Z = zs.reshape(M.shape)\n",
    "\n",
    "# create 3D axis object\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d', xlabel='Intercept',\n",
    "                     ylabel='Slope', zlabel='Loss')\n",
    "\n",
    "# plot loss hypersurface and contours\n",
    "ax.plot_surface(M, B, Z, rstride=1, cstride=1, alpha=0.3)\n",
    "ax.contour(M, B, Z, 20, alpha=0.5, offset=0, stride=30)\n",
    "\n",
    "# plot gradient descent \n",
    "ax.plot(betas[:, 0], betas[:, 1], losses, '.-')\n",
    "\n",
    "# set viewpoint\n",
    "ax.view_init(elev=10, azim=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-bishop",
   "metadata": {},
   "source": [
    "#### Newton-Raphson \n",
    "\n",
    "When the function $J$ to optimize is twice differentiable, the Newton-Raphson procedure consists in using $\\| \\nabla^2 J(\\boldsymbol\\beta^{(t)}) \\|$ as learning rate at update $t$ :\n",
    "\n",
    "$$\\boldsymbol\\beta^{(t+1)} = \\boldsymbol\\beta^{(t)} - \\|\\nabla^2 J(\\boldsymbol\\beta^{(t)})\\|.$$\n",
    "\n",
    "In our case, $\\|\\nabla^2 J(\\boldsymbol\\beta)\\| = \\|X^\\top X\\|= \\sigma_{\\max}$, where $\\sigma_{\\max}$ is the largest eigenvalue of $X^\\top X$.\n",
    "\n",
    "The learning rate suggested by this procedure is constant and equal to $\\frac1{\\sigma_{\\max}}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-reasoning",
   "metadata": {},
   "source": [
    "__Instruction:__ Define `learning_rates_list` below to compare several learning rates, including $\\frac1{\\sigma_{\\max}}$ and rates both lower and smaller than $\\frac1{\\sigma_{\\max}}$. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the eigenvalues of X'X\n",
    "eigens = np.linalg.eigvals(X_bt.T.dot(X_bt)/X_bt.shape[0])\n",
    "sigma_max = np.max(eigens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates_list = # TODO\n",
    "for lr in learning_rates_list:\n",
    "    # Run the batch gradient descent\n",
    "    losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.plot(np.arange(len(losses)), losses, 'o-', label=\"Learning rate = %.2e\" % lr)\n",
    "    \n",
    "beta_star_loss = squared_loss(X_bt, y, beta_star)\n",
    "plt.plot([0, iterations], [beta_star_loss, beta_star_loss], 'k-', label=\"Minimum loss\")\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Batch gradient descent\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-specification",
   "metadata": {},
   "source": [
    "__Remark:__ Note that you can replace `plot` with `semilogy` to use a logarithmic scale on the y-axis and better see differences in the lower values of the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-professor",
   "metadata": {},
   "source": [
    "__Question:__ How does this relate to the suggested learning rate for the case where the gradient of $J$ is L-Lipschitz (seen in class)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-pressing",
   "metadata": {},
   "source": [
    "### 1.4 Minibatch Gradient Descent\n",
    "\n",
    "In larger applications, computing the full gradient can be expensive. __Minibatch gradient descent__ uses a subsample of the data of size $m \\ll n$ (called a __batch__) at each iteration. If the dataset is large, this is often sufficient to make an accurate descent step.\n",
    "\n",
    "Computing the gradient therefore goes from a complexity of $\\mathcal{O}(np)$ to $\\mathcal{O}(mp)$. In the training of deep neural networks, minibatch gradient descent (and its variants) is overwhelmingly the most popular approach.\n",
    "\n",
    "The most straightforward strategy to build batches is to iterate through the data and take the samples slice of $m$ samples by slice of $m$ samples. In our case, at the iteration that uses batch $k$, we compute the gradient as  \n",
    "$$\\nabla_{\\boldsymbol\\beta} J(\\beta) = \\frac1m X_{k:k+m}^\\top \\left(X_{k:k+m} \\boldsymbol\\beta - y_{k:k+m} \\right).$$\n",
    "\n",
    "An __epoch__ corresponds to having seen the entire data set once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-tiger",
   "metadata": {},
   "source": [
    "__Instruction:__ Compute the code below to implement minibatch gradient descent for the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-pregnancy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T17:39:08.231736Z",
     "start_time": "2019-03-27T17:39:08.198510Z"
    }
   },
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, y, batch_size=10, learning_rate=1e-1, max_iters=100, tol=1e-5):    \n",
    "    \"\"\"\n",
    "    Minibatch gradient descent procedure for a linear regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector.\n",
    "        \n",
    "    batch_size: int\n",
    "        The number of samples in a batch.\n",
    "        \n",
    "    learning_rate: float\n",
    "        The learning rate of the gradient descent.\n",
    "        \n",
    "    max_iters: int\n",
    "        The maximum number of iterations of the gradient descent.\n",
    "        \n",
    "    tol: float\n",
    "        The tolerance of the gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Random initialisation of beta\n",
    "    beta_current = np.random.rand(# TODO)\n",
    "    \n",
    "    # Keep track of the values of the loss and of beta at each epoch\n",
    "    losses = [squared_loss(X, y, beta_current)]\n",
    "\n",
    "    # Find total number of samples\n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    # Initialize index of start of batch\n",
    "    idx_start_of_batch = 0\n",
    "    for iter_idx in range(max_iters):\n",
    "        # find index of end of batch\n",
    "        idx_end_of_batch = min(idx_start_of_batch + batch_size, num_samples)\n",
    "        \n",
    "        # construct batch\n",
    "        X_batch = # TODO\n",
    "        y_batch = # TODO\n",
    "        \n",
    "        # find index of start of next batch \n",
    "        idx_start_of_batch = idx_end_of_batch if idx_end_of_batch < num_samples else 0\n",
    "        \n",
    "        # update beta_current\n",
    "        beta_current = # TODO\n",
    "\n",
    "        # check whether tolerance is reached\n",
    "        if # TODO: \n",
    "            print(\"Gradient descent stopped after %d iterations\" % (idx+1))\n",
    "            break\n",
    "    \n",
    "        # compute the loss and append to losses\n",
    "        losses.append(squared_loss(X, y, beta_current))\n",
    "\n",
    "    return(np.array(losses), beta_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-circle",
   "metadata": {},
   "source": [
    "#### Impact of the learning rate\n",
    "\n",
    "We will now plot the evolution of the loss as a function of the number of iterations. Larger dots (circles) will mark the epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-truck",
   "metadata": {},
   "source": [
    "__Instructions:__ Complete the code below to run the minibatch gradient descent for a few different choices of learning rates. Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values of learning rates to test\n",
    "lr_list = # TODO\n",
    "\n",
    "tolerance = 1e-4\n",
    "iterations = 20\n",
    "bs = 20\n",
    "\n",
    "# Loop over the learning rates\n",
    "for lr in lr_list:\n",
    "    # Run the minibatch gradient descent\n",
    "    losses, beta_mgd = minibatch_gradient_descent(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.plot(np.arange(len(losses)), losses, label=\"Learning rate = %.2e\" % lr)\n",
    "    # Mark loss after each epoch\n",
    "    n_iterations_per_epoch = int(np.ceil(n_samples/bs))\n",
    "    n_epochs = int(iterations/n_iterations_per_epoch)\n",
    "    plt.scatter([int(np.ceil(n_samples/bs))*idx for idx in range(n_epochs)], \n",
    "                [losses[int(np.ceil(n_samples/bs))*idx] for idx in range(n_epochs)])\n",
    "    \n",
    "plt.plot([0, iterations], [beta_star_loss, beta_star_loss], '-k', label=\"Minimum loss\")\n",
    "plot\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Minibatch gradient descent\")\n",
    "plt.legend(loc=(1.01, 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-oriental",
   "metadata": {},
   "source": [
    "#### Impact of the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-present",
   "metadata": {},
   "source": [
    "__Instructions:__ Now complete the code below to fix the learning rate and run the minibatch gradient descent for a few different choices of batch sizes. Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "tolerance = 1e-4\n",
    "iterations = 20\n",
    "\n",
    "batch_size_list = # TODO\n",
    "\n",
    "# Loop over the learning rates\n",
    "for bs in batch_size_list:\n",
    "    # Run the minibatch gradient descent\n",
    "    losses, beta_mgd = minibatch_gradient_descent(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.plot(np.arange(len(losses)), losses, label=\"Batch size = %d\" % bs)\n",
    "    # Mark loss after each epoch\n",
    "    n_iterations_per_epoch = int(np.ceil(n_samples/bs))\n",
    "    n_epochs = int(iterations/n_iterations_per_epoch)\n",
    "    plt.scatter([n_iterations_per_epoch*idx for idx in range(n_epochs)], \n",
    "                [losses[int(n_iterations_per_epoch)*idx] for idx in range(n_epochs)])\n",
    "    \n",
    "plt.plot([0, iterations], [beta_star_loss, beta_star_loss], '-k', label=\"Minimum loss\")\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Minibatch gradient descent\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-hacker",
   "metadata": {},
   "source": [
    "### 1.5 Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-discharge",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is a special case of minibatch gradient descent:\n",
    "* the batch size is set to 1\n",
    "* one cycles randomly through the samples.\n",
    "\n",
    "In the case of neural networks, which have a non-convex optimization problem, stochasticity also may help to avoid ending in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-relationship",
   "metadata": {},
   "source": [
    "__Instructions:__ Complete the implementation of the stochastic gradient descent below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-genesis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T17:39:08.231736Z",
     "start_time": "2019-03-27T17:39:08.198510Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learning_rate=1e-1, max_iters=20):    \n",
    "    \"\"\"\n",
    "    Stochastic gradient descent procedure for a linear regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector.\n",
    "        \n",
    "    learning_rate: float\n",
    "        The learning rate of the gradient descent.\n",
    "        \n",
    "    max_iters: int\n",
    "        The maximum number of iters of the gradient descent.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Random initialisation of beta\n",
    "    beta_current = np.random.rand(# TODO)\n",
    "    \n",
    "    # Keep track of the values of the loss and of beta at each epoch\n",
    "    losses = [squared_loss(X, y, beta_current)]\n",
    "    \n",
    "    # Vector of sample indices\n",
    "    num_samples = X.shape[0]\n",
    "    samples_indices = np.arange(num_samples)\n",
    "    \n",
    "    # Determine number of epochs\n",
    "    n_epochs = int(ceil(max_iters/num_samples))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle the samples\n",
    "        np.random.shuffle(samples_indices)\n",
    "        \n",
    "        # The last epoch may not be full\n",
    "        samples_indices = samples_indices[:min(num_samples, (max_iters-epoch*num_samples))]\n",
    "        \n",
    "        # Loop over the shuffled samples one by one\n",
    "        for idx, sample_idx in enumerate(samples_indices):\n",
    "            # update beta_current\n",
    "            beta_current = # TODO\n",
    "               \n",
    "            # compute the loss and append to losses \n",
    "            losses.append(squared_loss(X, y, beta_current))\n",
    "\n",
    "    return(np.array(losses), beta_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-cause",
   "metadata": {},
   "source": [
    "__Question:__ Why aren't we using the norm of the gradient as stopping criterion any longer? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-finnish",
   "metadata": {},
   "source": [
    "__Instruction:__ Complete the code below by chosing a learning rate `lr` and observe how the loss evolves as a function of the number of iterations. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stochastic gradient descent\n",
    "iterations = 200\n",
    "lr = # TODO\n",
    "losses, betas = stochastic_gradient_descent(X_bt, y, learning_rate=lr, max_iters=iterations)\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses)), losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "# Mark loss after each epoch\n",
    "n_epochs = int(iterations/n_samples)\n",
    "plt.scatter([n_samples*idx for idx in range(n_epochs+1)], [losses[n_samples*idx] for idx in range(n_epochs+1)])\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"empirical risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-corporation",
   "metadata": {},
   "source": [
    "#### Comparing the three gradient descents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-doubt",
   "metadata": {},
   "source": [
    "__Instructions:__ Complete and run the following code to compare the three gradient descents by looking at their loss as a function of the number of iterations. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = # TODO\n",
    "\n",
    "# Run batch gradient descent with Newton-Raphson learning rate\n",
    "lr = 1./sigma_max\n",
    "losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses)), losses, label=\"Batch gradient descent\")\n",
    "\n",
    "# Run minibatch gradient descent\n",
    "bs = # TODO\n",
    "lr = # TODO\n",
    "losses, beta_mgd = minibatch_gradient_descent(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses)), losses, label=\"Minibatch gradient descent\")\n",
    "    \n",
    "# Run stochastic gradient descent\n",
    "lr = # TODO\n",
    "losses, beta_sgd = stochastic_gradient_descent(X_bt, y, learning_rate=lr,  max_iters=iterations)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses)), losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "# Plot closed-form loss\n",
    "plt.plot([0, iterations], [beta_star_loss, beta_star_loss], '-k', label=\"Minimum loss\")\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Comparing gradient descents\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-stroke",
   "metadata": {},
   "source": [
    "Another way to look at this is to look at the loss as a function of the *number of epochs*, that is to say the number of times the algorithm looked at the entire data set.\n",
    "\n",
    "__Instructions:__ Complete and run the following code to compare the three gradient descents by looking at their loss as a function of the number of epochs. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "epochs = # TODO \n",
    "\n",
    "# Run batch gradient descent\n",
    "lr = 1./sigma_max\n",
    "losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=epochs, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.semilogy(np.arange(len(losses)), losses, label=\"Batch gradient descent\")\n",
    "\n",
    "# Run minibatch gradient descent\n",
    "bs = # TODO use the same value as for the comparison per number of iterations\n",
    "lr = # TODO use the same value as for the comparison per number of iterations\n",
    "n_iterations_per_epoch = int(np.ceil(n_samples/bs))\n",
    "iterations = epochs * n_iterations_per_epoch\n",
    "losses, beta_mgd = minibatch_gradient_descent(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses))/n_iterations_per_epoch, losses, label=\"Minibatch gradient descent\")\n",
    "    \n",
    "# Run stochastic gradient descent\n",
    "lr = # TODO use the same value as for the comparison per number of iterations\n",
    "iterations = epochs * n_samples\n",
    "losses, beta_sgd = stochastic_gradient_descent(X_bt, y, learning_rate=lr,  max_iters=iterations)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses))/n_samples, losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "# Plot closed-form loss\n",
    "plt.plot([0, epochs], [beta_star_loss, beta_star_loss], '-k', label=\"Minimum loss\")\n",
    "\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Comparing gradient descents\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-naples",
   "metadata": {},
   "source": [
    "### 1.6 Multivariate linear regression\n",
    "\n",
    "The code we have written so far can be applied to data sets with more than one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-plenty",
   "metadata": {},
   "source": [
    "__Instructions:__ Complete the following code to compare the three types of gradient descent for a larger dataset, with 30 features. Play with the learning rates and batch sizes. Note and discuss what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_features = 30\n",
    "X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_features, noise=50, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias trick\n",
    "X_bt = np.hstack((np.ones((n_samples, 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed-form solution\n",
    "beta_star = # TODO\n",
    "beta_star_loss = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal learning rate for batch\n",
    "lr_optim = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "epochs = 10 # TODO change this value if you wish\n",
    "\n",
    "# Run batch gradient descent\n",
    "lr = 1./sigma_max\n",
    "losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=epochs, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses)), losses, label=\"Batch gradient descent\")\n",
    "\n",
    "# Run minibatch gradient descent\n",
    "bs = 20 # TODO try changing this value \n",
    "lr = 5e-2 # TODO try changing this value \n",
    "n_iterations_per_epoch = int(np.ceil(n_samples/bs))\n",
    "iterations = epochs * n_iterations_per_epoch\n",
    "losses, beta_mgd = minibatch_gradient_descent(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses))/n_iterations_per_epoch, losses, label=\"Minibatch gradient descent\")\n",
    "    \n",
    "# Run stochastic gradient descent\n",
    "lr = 5e-3 # TODO try changing this value \n",
    "iterations = epochs * n_samples\n",
    "losses, beta_sgd = stochastic_gradient_descent(X_bt, y, learning_rate=lr,  max_iters=iterations)\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(losses))/n_samples, losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "# Plot closed-form loss\n",
    "plt.plot([0, epochs], [beta_star_loss, beta_star_loss], '-k', label=\"Minimum loss\")\n",
    "\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Comparing gradient descents\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-dublin",
   "metadata": {},
   "source": [
    "### 1.7 SGDRegressor in scikit-learn\n",
    "\n",
    "The `SGDRegressor` class of `scikit-learn` ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)) makes it possible to learn linear regression models with various losses and regularizers, using stochastic gradient descent. \n",
    "\n",
    "We can use it here instead of our hand-written procedures.\n",
    "\n",
    "__Instructions:__ Complete the code below to use `SGDRegressor` and compare it to our procedures. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of SGDRegressor\n",
    "regressor = linear_model.SGDRegressor(loss='squared_loss', \n",
    "                                      penalty='l2', alpha=1e-5, # you have to add a regularizer, so we use a very small regularization parameter\n",
    "                                      max_iter=20, tol=1e-4, fit_intercept=False, \n",
    "                                      learning_rate='constant', eta0=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn\n",
    "regressor.fit(X_bt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SGDRegressor stopped after %d epochs.\" % regressor.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the loss of the model learned by SGDRegressor with the minimum loss\n",
    "print(\"Closed-form loss: %.2e\" % beta_star_loss)\n",
    "print(\"SGDRegressor loss: %.2e\" % (squared_loss(X_bt, y, regressor.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "epochs = # TODO Use the same value as in Section 1.6\n",
    "\n",
    "# Run batch gradient descent\n",
    "lr = 1./sigma_max\n",
    "losses, betas = batch_gradient_descent(X_bt, y, learning_rate=lr, max_iters=epochs, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.semilogy(np.arange(len(losses)), losses, label=\"Batch gradient descent\")\n",
    "\n",
    "# Run minibatch gradient descent\n",
    "bs = # TODO Use the same value as in Section 1.6\n",
    "lr = # TODO Use the same value as in Section 1.6\n",
    "n_iterations_per_epoch = int(np.ceil(n_samples/bs))\n",
    "iterations = epochs * n_iterations_per_epoch\n",
    "losses, beta_mgd = minibatch_gradient_descent(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)\n",
    "# Plot loss\n",
    "plt.semilogy(np.arange(len(losses))/n_iterations_per_epoch, losses, label=\"Minibatch gradient descent\")\n",
    "    \n",
    "# Run stochastic gradient descent\n",
    "lr =  # TODO Use the same value as in Section 1.6\n",
    "iterations = epochs * n_samples\n",
    "losses, beta_sgd = stochastic_gradient_descent(X_bt, y, learning_rate=lr,  max_iters=iterations)\n",
    "# Plot loss\n",
    "plt.semilogy(np.arange(len(losses))/n_samples, losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "# Plot closed-form loss\n",
    "plt.semilogy([0, epochs], [beta_star_loss, beta_star_loss], '-k', label=\"Minimum loss\")\n",
    "\n",
    "# Plot SGDClassifier loss\n",
    "plt.semilogy([0, epochs], [beta_star_loss, beta_star_loss], label=(\"SGDClassifier loss after %d epochs\" % regressor.n_iter_))\n",
    "\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Comparing gradient descents\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-berkeley",
   "metadata": {},
   "source": [
    "## 2. Logistic regression\n",
    "\n",
    "We will now work on binary classification with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-participant",
   "metadata": {},
   "source": [
    "### 2.1 Generating data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_features = 100\n",
    "X, y = datasets.make_classification(n_samples=n_samples, n_features=n_features, n_informative=np.int(n_features/10), random_state=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-continent",
   "metadata": {},
   "source": [
    "Let us standardize the data (each feature will be rescaled to have a mean of 0 and a standard deviation of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias trick\n",
    "X_bt = np.hstack((np.ones((n_samples, 1)), X_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-happiness",
   "metadata": {},
   "source": [
    "### 2.2 Optimization problem\n",
    "\n",
    "In logistic regression, the model is \n",
    "$\\mathbb{P}(Y = 1|X) = \\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}),$ with $Y$ being a binary random variable from which the outputs are drawn, and $X$ a $p$-dimensional random vector from which the features are drawn.\n",
    "$\\sigma: u \\mapsto \\frac1{1+e^{-u}}$ is the __sigmoid__ or __logistic__ function.\n",
    "\n",
    "The loss function is the __logistic loss__: $$\\mathcal{L}(y, f(\\boldsymbol{x})) = - y \\log(f(\\boldsymbol{x})- (1-y) \\log(1-f(\\boldsymbol{x})) $$\n",
    "\n",
    "Hence our goal is to minimize\n",
    "$$J(\\boldsymbol\\beta) = \\frac1n \\sum_{i=1}^n - y_i \\log(\\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i))- (1-y_i) \\log(1-\\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i))  $$\n",
    "\n",
    "Because $\\sigma^\\prime(u) = \\sigma(u) (1-\\sigma(u))$, we can easily obtain the following gradient for $J$:\n",
    "$$\\nabla J(\\boldsymbol\\beta) = - \\frac1n \\sum_{i=1}^n \\left(y_i - \\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i) \\right) \\boldsymbol{x}_i$$\n",
    "\n",
    "Because we do not know how to solve \n",
    "$$\\frac1n \\sum_{i=1}^n \\left(y_i - \\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i) \\right) \\boldsymbol{x}_i = 0,$$\n",
    "we do not have a closed-form solution.\n",
    "\n",
    "However we can use gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-housing",
   "metadata": {},
   "source": [
    "Here are an implementation of the evaluation of the logistic loss and of the gradient of the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    return (1./(1+np.exp(-u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(X, y, beta):\n",
    "    \"\"\"\n",
    "    Logistic loss of a logistic regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector (0/1).\n",
    "        \n",
    "    beta: (n_features, ) numpy array\n",
    "        The weight vector of the logistic regression.    \n",
    "    \"\"\"\n",
    "    where_y_pos = np.where(y)[0]\n",
    "    loss_pos = - np.sum(np.log(sigmoid(X[where_y_pos, :].dot(beta))))\n",
    "    where_y_neg = np.where(y==0)[0]\n",
    "    loss_neg = - np.sum(np.log(1 - sigmoid(X[where_y_neg, :].dot(beta))))\n",
    "    return (loss_pos + loss_neg)/np.size(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient_logistic(X, y, b_value):\n",
    "    \"\"\" \n",
    "    Gradient of the empirical risk of the logistic regression.\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    diff = sigmoid(X.dot(b_value)) - y \n",
    "    return np.sum(np.multiply(diff, X.T), axis=1)/num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-password",
   "metadata": {},
   "source": [
    "### 2.2 Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-cambridge",
   "metadata": {},
   "source": [
    "__Instruction:__ Complete the following code to implement the batch gradient descent procedure for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-baking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T17:38:54.314594Z",
     "start_time": "2019-03-27T17:38:54.288955Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent_logistic(X, y, learning_rate=1e-1, max_iters=30, tol=1e-2):\n",
    "    \"\"\"\n",
    "    Batch gradient descent procedure for a logistic regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector.\n",
    "        \n",
    "    learning_rate: float\n",
    "        The learning rate of the gradient descent.\n",
    "        \n",
    "    max_iters: int\n",
    "        The maximum number of iterations of the gradient descent.\n",
    "        \n",
    "    tol: float\n",
    "        The tolerance of the gradient descent.\n",
    "        \n",
    "    Returns:\n",
    "    losses: (n_iters, ) numpy array \n",
    "        The loss at each iteration.\n",
    "    \n",
    "    beta_vector: (n_features, ) numpy array\n",
    "        The fitted regression vector (i.e. the solution).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return(losses, beta_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-destruction",
   "metadata": {},
   "source": [
    "Now run the code below to run the batch gradient descent on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = 200\n",
    "lr = 5.\n",
    "losses, beta_final = batch_gradient_descent_logistic(X_bt, y, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses)\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"value of the loss\")\n",
    "plt.title(\"Batch gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-board",
   "metadata": {},
   "source": [
    "#### Quality of the model\n",
    "\n",
    "__Instructions:__ Complete the following code to compute the ROC curve of your model on the training data. Note again that here we're only trying to evaluate how well the model fits the data, and not whether it generalizes well. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gd = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, y_pred_gd)\n",
    "plt.plot(fpr, tpr, label=\"Batch gradient descent (AUC=%.3f)\" % metrics.auc(fpr, tpr))\n",
    "\n",
    "plt.xlabel(\"TPR\")\n",
    "plt.ylabel(\"FPR\")\n",
    "plt.title(\"ROC curve (on the train set)\")\n",
    "plt.legend(loc=(1.01, .9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-candidate",
   "metadata": {},
   "source": [
    "### 2.3 Comparing batch, minibatch and stochastic gradient descents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-alliance",
   "metadata": {},
   "source": [
    "#### Minibatch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-making",
   "metadata": {},
   "source": [
    "__Instructions:__ Complete the code below that implements minibatch gradient descent for the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-eligibility",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T17:39:08.231736Z",
     "start_time": "2019-03-27T17:39:08.198510Z"
    }
   },
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent_logistic(X, y, batch_size=10, learning_rate=1e-1, max_iters=100, tol=1e-5):    \n",
    "    \"\"\"\n",
    "    Minibatch gradient descent procedure for a logistic regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector.\n",
    "        \n",
    "    batch_size: int\n",
    "        The number of samples in a batch.\n",
    "        \n",
    "    learning_rate: float\n",
    "        The learning rate of the gradient descent.\n",
    "        \n",
    "    max_iters: int\n",
    "        The maximum number of iterations of the gradient descent.\n",
    "        \n",
    "    tol: float\n",
    "        The tolerance of the gradient descent\n",
    "        \n",
    "    Returns:\n",
    "    losses: (n_iters, ) numpy array \n",
    "        The loss at each iteration.\n",
    "    \n",
    "    beta_vector: (n_features, ) numpy array\n",
    "        The fitted regression vector (i.e. the solution).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return(losses, beta_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-senator",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-passenger",
   "metadata": {},
   "source": [
    "__Instructions:__ Complete the code below that implements stochastic gradient descent for the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-jordan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T17:39:08.231736Z",
     "start_time": "2019-03-27T17:39:08.198510Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_logistic(X, y, learning_rate=1e-1, max_iters=20):    \n",
    "    \"\"\"\n",
    "    Stochastic gradient descent procedure for a logistic regression.\n",
    "    Parameters:\n",
    "    X: (n_samples, n_features) numpy array\n",
    "        The data matrix.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        The output/target vector.\n",
    "        \n",
    "    learning_rate: float\n",
    "        The learning rate of the gradient descent.\n",
    "        \n",
    "    max_iters: int\n",
    "        The maximum number of iters of the gradient descent.\n",
    "        \n",
    "    Returns:\n",
    "    losses: (n_iters, ) numpy array \n",
    "        The loss at each iteration.\n",
    "    \n",
    "    beta_vector: (n_features, ) numpy array\n",
    "        The fitted regression vector (i.e. the solution).\n",
    "        \n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return(losses, beta_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-oasis",
   "metadata": {},
   "source": [
    "#### Comparison per number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-tsunami",
   "metadata": {},
   "source": [
    "__Instruction:__ Complete the code below to compare the evolution of the loss, for the batch, minibatch, and stochastic gradient descents, as a function of the number of iterations. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = 50 # TODO change this value if you wish\n",
    "\n",
    "# Run batch gradient descent\n",
    "lr = 1. # TODO try changing this value\n",
    "bgd_losses, bgd_beta = batch_gradient_descent_logistic(X_bt, y, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run minibatch gradient descent\n",
    "bs = 50 # TODO try changing this value\n",
    "lr = 0.1 # TODO try changing this value\n",
    "mgd_losses, mgd_beta = minibatch_gradient_descent_logistic(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stochastic gradient descent\n",
    "lr = 1e-2 # TODO try changing this value\n",
    "sgd_losses, sgd_beta = stochastic_gradient_descent_logistic(X_bt, y, learning_rate=lr, max_iters=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot batch gradient loss\n",
    "plt.plot(np.arange(len(bgd_losses)), bgd_losses, label=\"Batch gradient descent\")\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(mgd_losses)), mgd_losses, label=\"Minibatch gradient descent\")\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(np.arange(len(sgd_losses)), sgd_losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"value of the loss\")\n",
    "plt.title(\"Comparing gradient descents\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-parks",
   "metadata": {},
   "source": [
    "#### Comparison per number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-aggregate",
   "metadata": {},
   "source": [
    "__Instruction:__ Complete the code below to compare the evolution of the loss, for the batch, minibatch, and stochastic gradient descents, as a function of the number of epochs. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "epochs = 5 # TODO change this is you wish\n",
    "\n",
    "# Run batch gradient descent\n",
    "lr = # TODO use the same value as for the comparison per number of iterations\n",
    "bgd_losses, bgd_beta = batch_gradient_descent_logistic(X_bt, y, learning_rate=lr, max_iters=epochs, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run minibatch gradient descent\n",
    "bs = # TODO use the same value as for the comparison per number of iterations\n",
    "lr = # TODO use the same value as for the comparison per number of iterations\n",
    "n_iterations_per_epoch = int(np.ceil(n_samples/bs))\n",
    "iterations = epochs * n_iterations_per_epoch\n",
    "mgd_losses, mgd_beta = minibatch_gradient_descent_logistic(X_bt, y, batch_size=bs, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stochastic gradient descent\n",
    "lr = # TODO use the same value as for the comparison per number of iterations\n",
    "iterations = epochs * n_samples\n",
    "sgd_losses, sgd_beta = stochastic_gradient_descent_logistic(X_bt, y, learning_rate=lr, max_iters=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.arange(len(bgd_losses)), bgd_losses, label=\"Batch gradient descent\")\n",
    "plt.semilogy(np.arange(len(mgd_losses))/n_iterations_per_epoch, mgd_losses, label=\"Minibatch gradient descent\")\n",
    "plt.semilogy(np.arange(len(sgd_losses))/n_samples, sgd_losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Comparing gradient descents\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-advocacy",
   "metadata": {},
   "source": [
    "### 2.4 SGDClassifier in scikit-learn\n",
    "\n",
    "Use the `SGDClassifier` class of scikit-learn ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)) instead of the above hand-written procedures. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of SGDClassifier\n",
    "classifier = linear_model.SGDClassifier(# TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn\n",
    "classifier.fit(X_bt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SGDClassifier stopped after %d epochs.\" % classifier.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the loss of the model learned by SGDClassifier with the minimum loss\n",
    "sgd_loss = logistic_loss(X_bt, y, classifier.coef_.reshape((X_bt.shape[1], )))\n",
    "print(\"SGDClassifier loss: %.2e\" % sgd_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.arange(len(bgd_losses)), bgd_losses, label=\"Batch gradient descent\")\n",
    "plt.semilogy(np.arange(len(mgd_losses))/n_iterations_per_epoch, mgd_losses, label=\"Minibatch gradient descent\")\n",
    "plt.semilogy(np.arange(len(sgd_losses))/n_samples, sgd_losses, label=\"Stochastic gradient descent\")\n",
    "\n",
    "plt.semilogy([0, epochs], [sgd_loss, sgd_loss], 'k--', label=('SGDClassifier after %d epochs' % classifier.n_iter_))\n",
    "\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"empirical risk\")\n",
    "plt.title(\"Comparing gradient descents\")\n",
    "plt.legend(loc=(1.01, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-anchor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-cream",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
